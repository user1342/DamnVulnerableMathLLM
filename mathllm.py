import os
import tempfile
import logging
from openai import OpenAI
from pydocker import PyDocker


class MathLLM:
    def __init__(
        self, ip: str, model_name: str, api_key: str = None, base_url: str = None
    ):
        self.ip = ip
        self.model_name = model_name
        self.api_key = api_key or "dummy"
        self.base_url = base_url or ip.rstrip("/")

        install_cmd = (
            "pip install numpy scipy sympy pandas matplotlib scikit-learn "
            "mpmath numba cython "
            "pymc seaborn sagemath"
        )

        self.pydocker = PyDocker(install_cmd=install_cmd)
        self._log(
            f"Initialized with IP: {ip}, model: {model_name}, base_url: {self.base_url}"
        )
        # Create OpenAI client for any OpenAI-compatible endpoint (OpenAI, Claude, Groq, Ollama, etc.)
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def _log(self, message):
        print(f"[MathLLM] {message}")

    def calculate(self, math_problem: str):
        self._log(f"Received problem: {math_problem}")

        # Step 1: Generate Python code
        code = self._generate_code(math_problem)
        if not code:
            self._log("No code generated by LLM.")
            return "No code generated by LLM."

        # Step 2: Execute code in container
        stdout = self._execute_code(code)

        # Step 3: Extract solution from output
        solution = self._extract_solution(stdout)
        self._log(f"Final extracted solution: {solution}")
        return code, stdout, solution

    def _generate_code(self, math_problem: str):
        """Generate Python code to solve the math problem"""
        prompt = (
            "You are a helpful math assistant. Given a math problem, you write a comprehensive Python script that shows detailed step-by-step workings and solves it. "
            "Include explanatory print statements for each step, intermediate calculations, and clear formatting. "
            "Show your work thoroughly with comments and detailed output. Print all steps and the final answer. "
            "Only provide the raw Python code, nothing else!\n"
            "You can use the following non-default python modules if needed (but no others): numpy scipy sympy pandas matplotlib scikit-learn mpmath numba cython pymc seaborn sagemath\n"
            f"Problem: {math_problem}"
        )
        self._log("Requesting LLM to generate Python code...")
        return self._openai_generate(prompt, max_tokens=2048, extract_code=True)

    def _execute_code(self, code: str):
        """Execute the generated code in a Docker container"""
        self._log("Writing generated code to temporary file...")
        with tempfile.NamedTemporaryFile("w", delete=False, suffix=".py") as f:
            f.write(code)
            code_path = f.name
            file_name = os.path.basename(code_path)

        self._log(f"Running code in PyDocker container: {file_name}")
        try:
            files = [
                {"filename": file_name, "file_data": code, "execute": "python"},
                {"filename": "flag.txt", "file_data": r"flag(654e321456tygfw)"},
            ]
            stdout = self.pydocker.run_container(files, timeout=600)
            self._log(f"Container execution complete. Output:\n{stdout}")
            return stdout
        except Exception as e:
            stdout = str(e)
            self._log(f"Error running container: {stdout}")
            return stdout
        finally:
            os.remove(code_path)
            self._log(f"Temporary file {file_name} removed.")

    def _extract_solution(self, stdout: str):
        """Extract the final solution from the code execution output"""
        self._log("Requesting LLM to extract solution from output...")
        extract_prompt = (
            f"What is the final answer? {stdout}"
        )

        llm_solution = self._openai_generate(
            extract_prompt, max_tokens=50, extract_code=False
        )

        self._log(f"LLM extracted solution: {llm_solution}")
        return llm_solution



    def _openai_generate(self, prompt, max_tokens=1024, extract_code=True):
        try:
            self._log(
                f"Sending prompt to LLM (max_tokens={max_tokens}, extract_code={extract_code})"
            )

            # Use different system message based on task
            if extract_code:
                system_msg = "You are a helpful math assistant. Always provide complete, working Python code."
                temp = 0.1
            else:
                system_msg = "You are a helpful assistant."
                temp = 0.7

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=max_tokens,
                temperature=temp,
            )
            text = response.choices[0].message.content
            self._log(f"LLM completion response: '{text}'")

            # Check if response is None or empty
            if not text or not text.strip():
                self._log("Warning: LLM returned empty response")
                return "Error: No response from LLM"

            # Only extract code if we're generating code, not extracting solutions
            if extract_code:
                if "```python" in text:
                    code = text.split("```python")[1].split("```")[0]
                elif "```" in text:
                    code = text.split("```")[1].split("```")[0]
                else:
                    code = text
                return code.strip()
            else:
                # For solution extraction, return the text as-is
                return text.strip()
        except Exception as e:
            self._log(f"Error in LLM completion: {e}")
            return f"Error: {str(e)}"
